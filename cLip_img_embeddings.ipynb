{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201100a-ccf3-4946-aa17-8ec0fcbdd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc301e0-ca1a-46c2-a286-ec7b1323cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip  install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1c4b95-2d0a-40a1-901e-cb514c2217de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d8a5e6-391e-4d1d-b1dd-92cb9e0f9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing images\n",
    "folder_path = \"processed_images\"\n",
    "files = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a421764-68e0-4d72-8a79-c1fcfbacc328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch id: 0\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 1\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 2\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 3\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 4\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 5\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 6\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 7\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 8\n",
      "torch.Size([1000, 512])\n",
      "Batch id: 9\n",
      "torch.Size([1000, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "total_files = len(files)\n",
    "n_batch_iter = int(total_files/batch_size)\n",
    "desired_feature_dimension = 512\n",
    "\n",
    "# Initialize a tensor to accumulate image features\n",
    "all_image_features = torch.empty(0, desired_feature_dimension).to(device)  # Assuming desired_feature_dimension is known\n",
    "for i in range(n_batch_iter):\n",
    "    print(f\"Batch id: {i}\")\n",
    "    batch_sample = files[i*batch_size:(i*batch_size + batch_size)]\n",
    "    image_list = []    \n",
    "    for file in batch_sample:\n",
    "        image_list.append(Image.open(folder_path + '/' + file))\n",
    "    batch_image_input = torch.stack([preprocess(image) for image in image_list]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(batch_image_input)    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    print(image_features.shape)\n",
    "    all_image_features = torch.cat((all_image_features, image_features), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1082ed75-09a7-42d8-8ba7-5b77528b02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the tensor to a pickle file\n",
    "with open('processed_image_features.pkl', 'wb') as f:\n",
    "    pickle.dump({\"image_feature\":all_image_features, \"files\":files}, f)\n",
    "\n",
    "# Load the pickle file\n",
    "with open('processed_image_features.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2b6741-e4ef-4630-9063-1d1d437b8e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_feature'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cbc5922-3e8d-4a74-bfa5-6c36bbcefad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ba8740-7865-4b31-aa52-e69c4b523a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7861a30-0bf7-4c8a-9a64-1cf7f1fc3dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
